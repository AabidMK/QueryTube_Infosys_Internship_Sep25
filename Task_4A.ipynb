{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "TaIVxypNUwrG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data - ensure master_task1_datset.csv is uploaded to your Colab environment\n",
        "file_path = 'master_task1_datset.csv'\n",
        "df = pd.read_csv(file_path)\n"
      ],
      "metadata": {
        "id": "pM_tE9mcVJ3g"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Text Cleaning Function (for Title, Description, and Uniqueness Check) ---\n",
        "def clean_text(text):\n",
        "    if pd.isna(text) or text is None:\n",
        "        return ''\n",
        "    text = str(text)\n",
        "\n",
        "    # 1. Remove HTML tags (<br>, <i>)\n",
        "    text = re.sub(r'<[^>]+>', '', text)\n",
        "\n",
        "    # 2. Remove Emojis\n",
        "    emoji_pattern = re.compile(\n",
        "        \"[\"\n",
        "        \"\\U0001F600-\\U0001F64F\" \"\\U0001F300-\\U0001F5FF\"\n",
        "        \"\\U0001F680-\\U0001F6FF\" \"\\U0001F900-\\U0001F9FF\"\n",
        "        \"\\U0001FA00-\\U0001FAFF\" \"\\U00002702-\\U000027B0\"\n",
        "        \"\\U000024C2-\\U0001F251\"\n",
        "        \"]+\", flags=re.UNICODE)\n",
        "    text = emoji_pattern.sub(r'', text)\n",
        "\n",
        "    # 3. Remove Special characters/Punctuation\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
        "\n",
        "    # 4. Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # 5. Strip extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "PV3ry0WqVVdf"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply text cleaning to 'title' and 'description'\n",
        "df['cleaned_title'] = df['title'].apply(clean_text)\n",
        "df['cleaned_description'] = df['description'].apply(clean_text)\n"
      ],
      "metadata": {
        "id": "CZnY2rV_VcJr"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. Convert ISO 8601 Duration to Total Seconds (using regex) ---\n",
        "def iso8601_to_seconds(duration_str):\n",
        "    if pd.isna(duration_str) or not isinstance(duration_str, str):\n",
        "        return 0\n",
        "\n",
        "    # Regex to extract components (H, M, S) from ISO 8601 format like 'PT#H#M#S'\n",
        "    regex = r'P(T)?(?:(\\d+)H)?(?:(\\d+)M)?(?:(\\d+)S)?'\n",
        "    match = re.match(regex, duration_str)\n",
        "\n",
        "    if not match:\n",
        "        return 0\n",
        "\n",
        "    # Extracted groups are Hour, Minute, Second (None if not present)\n",
        "    hours, minutes, seconds = [int(g) if g else 0 for g in match.groups()[1:]]\n",
        "\n",
        "    total_seconds = (hours * 3600) + (minutes * 60) + seconds\n",
        "    return total_seconds"
      ],
      "metadata": {
        "id": "fAcJPRnfVme4"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['duration_seconds'] = df['duration'].apply(iso8601_to_seconds)\n"
      ],
      "metadata": {
        "id": "nmBfu5BhVpux"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. Convert Count Columns to Numeric Integers (using camelCase names for master_task1_datset.csv) ---\n",
        "count_cols = ['viewCount', 'likeCount', 'commentCount', 'channel_subscriberCount', 'channel_videoCount']\n",
        "\n",
        "for col in count_cols:\n",
        "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "    df[col] = df[col].fillna(0)\n",
        "    df[col] = df[col].astype(int)"
      ],
      "metadata": {
        "id": "3VcWmaLAVsdg"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. Process Tags Column ---\n",
        "df['tags_processed'] = df['tags'].fillna('').astype(str).str.lower().str.strip()\n"
      ],
      "metadata": {
        "id": "QmWL-JTMVz3n"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5. Uniqueness Check (check for duplicates across video_id) ---\n",
        "title_uniqueness_check = df.groupby('cleaned_title')['id'].nunique().reset_index(name='unique_video_id_count')\n",
        "duplicates_by_title = title_uniqueness_check[title_uniqueness_check['unique_video_id_count'] > 1]"
      ],
      "metadata": {
        "id": "s4w5blqTV4Bz"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Display Results and Finalize DataFrame ---\n",
        "print(\"--- Data Types After Cleaning and Conversion ---\")\n",
        "print(df[count_cols + ['duration_seconds']].dtypes)\n",
        "\n",
        "print(\"\\n--- Head of Transformed Data ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVKlY3dDXhKb",
        "outputId": "f166e2c4-9d79-48c9-f727-cb76322d3b8c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Data Types After Cleaning and Conversion ---\n",
            "viewCount                  int64\n",
            "likeCount                  int64\n",
            "commentCount               int64\n",
            "channel_subscriberCount    int64\n",
            "channel_videoCount         int64\n",
            "duration_seconds           int64\n",
            "dtype: object\n",
            "\n",
            "--- Head of Transformed Data ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying original and new columns for comparison\n",
        "print(df[['title', 'cleaned_title', 'duration', 'duration_seconds', 'viewCount', 'likeCount']].head())\n",
        "\n",
        "print(\"\\n--- Titles with Multiple Unique Video IDs (Duplicates Check) ---\")\n",
        "print(duplicates_by_title)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6NbTXKDXjEk",
        "outputId": "28c68d36-5e08-404a-d00d-d3f4dd71f5ac"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               title  \\\n",
            "0  Data Analyst Jobs are COOKED ð¤¯ This is how ...   \n",
            "1   H1B is $100K now: How will Big Tech Survive ð¤¯   \n",
            "2                    Data Engineer vs Data Scientist   \n",
            "3  Is Big Tech a trap? Why she quit her $200K Sof...   \n",
            "4                   5 Data Analyst Projects You NEED   \n",
            "\n",
            "                                       cleaned_title  duration  \\\n",
            "0  data analyst jobs are cooked this is how you s...   PT1M17S   \n",
            "1          h1b is 100k now how will big tech survive   PT1M34S   \n",
            "2                    data engineer vs data scientist     PT57S   \n",
            "3  is big tech a trap why she quit her 200k softw...  PT22M21S   \n",
            "4                   5 data analyst projects you need     PT58S   \n",
            "\n",
            "   duration_seconds  viewCount  likeCount  \n",
            "0                77       7567        402  \n",
            "1                94       8017        196  \n",
            "2                57       2392        130  \n",
            "3              1341       2385         69  \n",
            "4                58      11224        625  \n",
            "\n",
            "--- Titles with Multiple Unique Video IDs (Duplicates Check) ---\n",
            "Empty DataFrame\n",
            "Columns: [cleaned_title, unique_video_id_count]\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the final cleaned DataFrame\n",
        "df.to_csv('master_task1_fully_cleaned.csv', index=False)"
      ],
      "metadata": {
        "id": "wn4A87OhXlaq"
      },
      "execution_count": 24,
      "outputs": []
    }
  ]
}