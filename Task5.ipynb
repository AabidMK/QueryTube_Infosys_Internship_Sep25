{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbfd3a72"
      },
      "source": [
        "%pip install chromadb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Import required libraries ===\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import chromadb\n",
        "from chromadb.config import Settings"
      ],
      "metadata": {
        "id": "RKHhwFJvzIXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === File paths ===\n",
        "metadata_path = \"master_task1_fully_cleaned.csv\"\n",
        "transcripts_path = \"master_task2_clean_transcript_dataset.csv\"\n",
        "\n",
        "merged_output_csv = \"merged_dataset.csv\"\n",
        "embeddings_output_csv = \"dataset_with_embeddings.csv\""
      ],
      "metadata": {
        "id": "cI_mBS2r0OzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Step 1: Load and Merge Metadata + Transcripts ===\n",
        "print(\"üîπ Loading CSV files...\")\n",
        "meta_df = pd.read_csv(metadata_path)\n",
        "trans_df = pd.read_csv(transcripts_path)\n",
        "print(\"üîπ Merging datasets on 'video_id'...\")\n",
        "merged_df = pd.merge(meta_df, trans_df, on='id', how='inner')"
      ],
      "metadata": {
        "id": "qeFNB0Os1QUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Step 2: Clean Data ===\n",
        "print(\"üîπ Cleaning data...\")\n",
        "merged_df.dropna(subset=['transcript'], inplace=True)\n",
        "merged_df = merged_df[merged_df['transcript'].str.strip() != \"\"]"
      ],
      "metadata": {
        "id": "9FRNFhg52v8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the merged clean dataset\n",
        "merged_df.to_csv(merged_output_csv, index=False)\n",
        "print(f\"‚úÖ Merged dataset saved as {merged_output_csv}\")"
      ],
      "metadata": {
        "id": "Az372dLY23xW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Step 3: Generate Semantic Embeddings ===\n",
        "print(\"üîπ Generating embeddings using 'all-mpnet-base-v2' (high accuracy)...\")\n",
        "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')"
      ],
      "metadata": {
        "id": "18qJEveN3LQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine title + transcript for rich context\n",
        "merged_df['text_for_embedding'] = merged_df['title'].fillna('') + \" \" + merged_df['transcript'].fillna('')\n"
      ],
      "metadata": {
        "id": "Kk22LG6M3V4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate embeddings (batched for speed)\n",
        "embeddings = model.encode(\n",
        "    merged_df['text_for_embedding'].tolist(),\n",
        "    show_progress_bar=True,\n",
        "    batch_size=32,\n",
        "    convert_to_numpy=True,\n",
        "    normalize_embeddings=True\n",
        ")"
      ],
      "metadata": {
        "id": "gpPxb70a3dk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add embeddings as strings for CSV storage\n",
        "merged_df['embedding'] = [\",\".join(map(str, emb)) for emb in embeddings]\n"
      ],
      "metadata": {
        "id": "_W6LsNOB5aP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save dataset with embeddings\n",
        "merged_df.to_csv(embeddings_output_csv, index=False)\n",
        "print(f\"‚úÖ Embeddings saved to {embeddings_output_csv}\")"
      ],
      "metadata": {
        "id": "nUbCdGyx5eYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Step 4: Store in ChromaDB ===\n",
        "print(\"üîπ Initializing ChromaDB and adding data...\")\n",
        "\n",
        "client = chromadb.Client(Settings(\n",
        "    persist_directory=\"chroma_storage\",  # directory for persistent storage\n",
        "    anonymized_telemetry=False\n",
        "))\n",
        "\n",
        "collection = client.get_or_create_collection(name=\"video_embeddings\")"
      ],
      "metadata": {
        "id": "ghV4Ae7V5kqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert embedding strings back to lists of floats before adding\n",
        "embedding_vectors = [np.fromstring(e, sep=\",\") for e in merged_df['embedding']]\n"
      ],
      "metadata": {
        "id": "C0ZHi4-35yxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove duplicate IDs before adding to ChromaDB\n",
        "merged_df_unique = merged_df.drop_duplicates(subset=['id'])\n",
        "\n",
        "# Add data to ChromaDB\n",
        "collection.add(\n",
        "    ids=merged_df_unique['id'].astype(str).tolist(),\n",
        "    embeddings=[embedding_vectors[i] for i in merged_df_unique.index], # Select embeddings corresponding to unique IDs\n",
        "    metadatas=[\n",
        "        {\n",
        "            \"video_id\": row['id'],\n",
        "            \"title\": row['title'],\n",
        "            \"transcript\": row['transcript']\n",
        "        }\n",
        "        for _, row in merged_df_unique.iterrows()\n",
        "    ],\n",
        "    documents=merged_df_unique['text_for_embedding'].tolist()\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Embeddings and metadata successfully stored in ChromaDB!\")"
      ],
      "metadata": {
        "id": "vzMw8GPb6Axg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Step 5: Example Query (Fixed) ===\n",
        "print(\"\\nüîç Running example semantic query...\")\n",
        "\n",
        "query = \"How to use AI for video summarization\"\n",
        "\n",
        "# Use the same model that generated stored embeddings\n",
        "query_embedding = model.encode([query], normalize_embeddings=True)\n",
        "\n",
        "results = collection.query(\n",
        "    query_embeddings=query_embedding.tolist(),\n",
        "    n_results=3\n",
        ")\n",
        "\n",
        "print(\"\\nüîç Example Search Results:\")\n",
        "for i, doc in enumerate(results['documents'][0]):\n",
        "    print(f\"\\nResult {i+1}:\")\n",
        "    print(\"Title:\", results['metadatas'][0][i]['title'])\n",
        "    print(\"Transcript snippet:\", doc[:300])"
      ],
      "metadata": {
        "id": "TcYRKEbO6WdJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}